<html>
	<head>
		<title>Задача 14. "Простое тестовое задание"</title>
	</head>
	<body>
		<h1>Сравнение открытых OLAP-систем <br>Big Data: ClickHouse, Druid и Pinot</h1>
			<p>Оригинал - https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-
				<br>for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7</p>
			<p><a href="">ClickHouse, Druid</a> и <a href="">Pinot</a> – три открытых хранилища данных, которые позволяют 
				<br>выполнять аналитические запросы на больших объемах данных с интерактивными 
				<br>задержками. Эта статья - перевод <a href="">подробного сравнения</a>, выполненного Романом Левентовым.
		<h3>Источники информации</h3>
			<p>Подробности реализации <b>ClickHouse</b> стали мне известны от <a href="">Алексея Зателепина</a>, 
				<br>одного из <b>ключевых разработчиков проекта</b>. Доступная на английском 
				<br>документация достаточно скудна – наилучшим источником информации служат 
				<br>последние четыре секции <a href="">данной страницы документации</a>.</p>
			<p><b>Я сам участвую в развитии Druid</b>, но у меня нет личной заинтересованности в этой 
				<br>системе - по правде говоря, скорее всего в ближайшее время я перестану заниматься 
				<br>её разработкой. Поэтому читатели могут рассчитывать на отсутствие какой-либо 
				<br>предвзятости.</p>
			<p>Всё, что я буду далее писать про <b>Pinot</b>, основывается на странице <a href="">Архитектура в вики 
				<br>Pinot</a>, а также на других страницах вики в разделе “Проектная документация”. 
				<br>Последний раз они обновлялись в июне 2017 года - больше, чем полгода назад.</p>
			<p>Рецензентами оригинальной статьи стали Алексей Зателепин и <a href="">Виталий Людвиченко</a> 
				<br>(разработчики ClickHouse), <a href="">Жан Мерлино</a> (самый активный разработчик Druid), <a href="">Кишор 
				<br>Гопалакришна</a> (архитектор Pinot) и <a href="">Жан-Француа Им</a> (разработчик Pinot). Мы 
				<br>присоединяемся к благодарности автора и полагаем, что это многократно повышает 
				<br>авторитетность статьи. </p>
			<p><b>Предупреждение</b>: статья достаточно большая, поэтому вполне возможно вы захотите 
				<br>ограничиться прочтением раздела “Заключение” в конце.</p>
		<h2>Сходства между системами</h2>
		<h3>Связанные данные и вычисления</h3>
			<p>На фундаментальном уровне, ClickHouse, Druid и Pinot похожи, поскольку они 
				<br>хранят данные и выполняют обработку запросов на одних и тех же узлах, уходя от 
				<br>“разъединенной” архитектуры BigQuery. Недавно я уже описывал несколько 
				<br>наследственных проблем со связанной архитектурой в случае Druid [<a href="">1, 2</a>]. Открытого 
				<br>эквивалента для BigQuery на данный момент не существует (за исключением, разве 
				<br>что, <a href="">Drill</a>?) Возможным подходам к построению подобных открытых систем посвящена 
				<br><a href="">другая статье в моем блоге</a>.</p>
		<h3>Отличия от Big Data SQL-систем: индексы и статическое 
			<br>распределение данных</h3>
			<p>Рассматриваемые в этой статье системы <b>выполняют запросы быстрее</b>, чем 
				<br>системы Big Data из семейства класса SQL-on-Hadoop: Hive, Impala, Presto и Spark, 
				<br>даже когда последние получают доступ к данным, хранящимся в колоночном формате 
				<br>- к примеру, Parquet или Kudu. Это происходит потому, что в ClickHouse, Druid и Pinot:</p>
					<ul>
						<li>Имеется <b>свой собственный формат для хранения данных с индексами</b>, и 
							<br>они тесно интегрированы с движками обработки запросов. Системы класса 
							<br>SQL-on-Hadoop обычно можно назвать агностиками относительно форматов 
							<br>данных и поэтому они менее “навязчивы” в бэкендах Big Data.</li>
						<li><b>Данные распределены относительно “статично”</b> между узлами, и при 
							<br>распределенном выполнении запроса это можно использовать. Обратная 
							<br>сторона медали при этом в том, что ClickHouse, Druid и Pinot <b>не поддерживают 
							<br>запросы, которые требуют перемещения большого количества данных</b> 
							<br>между узлами - к примеру, join между двумя большими таблицами.</li>
					</ul>
		<h3>Отсутствие точечных обновлений и удалений</h3>
			<p>Находясь на другой стороне спектра баз данных, ClickHouse, Druid и Pinot <b>не 
				<br>поддерживают точечные обновления и удаления</b>, в противоположность 
				<br>колоночным системам вроде Kudu, InfluxDB и Vertica (?). Это даёт ClickHouse, Druid и 
				<br>Pinot возможность производить более эффективное колоночное сжатие и более 
				<br>агрессивные индексы, что означает <b>большую эффективность использования 
				<br>ресурсов</b> и быстрое выполнение запросов.</p>
			<p>Разработчики ClickHouse в Yandex планируют начать поддерживать <a href="">обновления и 
				<br>удаления в будущем</a>, но я не уверен, будут ли это “настоящие” точечные запросы или 
				<br>обновления/удаления диапазонов данных.</p>
		<h3>Поглощение в стиле Big Data</h3>
			<p>Все три системы поддерживают потоковое поглощение данных из Kafka. Druid и Pinot 
				<br>поддерживают потоковую передачу данных стриминг в <a href="">Лямбда-стиле</a> и пакетное 
				<br>поглощение одних и тех же данных. ClickHouse поддерживает пакетные вставки 
				<br>напрямую, поэтому ему не требуется отдельная система пакетного поглощения 
				<br>подобная той, что используется в Druid и Pinot. Если вас интересуют подробности, то 
				<br>их вы сможете найти далее.</p>
		<h3>Проверено на крупном масштабе</h3>
			<p>Все три системы проверены на работоспособность в крупных масштабах: в 
				<br><a href="">Yandex.Metrica работает кластер ClickHouse</a>, состоящий из примерно десятка тысяч 
				<br>ядер CPU. В Metamarkets используется <a href="">кластер Druid аналогичного размера</a>. Один 
				<br>кластер Pinot в LinkedIn включает в себя <a href="">“тысячи машин”</a>.</p>
		<h3>Незрелость</h3>
			<p>Все рассматриваемые в статье системы являются <b>незрелыми по меркам открытых 
				<br>enterprise-систем Big Data</b>. Однако, скорее всего они незрелы не более, чем 
				<br>среднестатистическая открытая система Big Data - но это совсем другая история. В 
				<br>ClickHouse, Druid и Pinot недостает некоторых очевидных оптимизаций и 
				<br>функциональности, и они кишат багами (насчет ClickHouse и Pinot я не уверен на все 
				<br>100%, но не вижу причин, по которым они в этом плане были бы лучше Druid).</p>
			<p>Это плавно подводит нас к следующему важному разделу.</p>
		<h3>Про сравнение производительности и выбор системы</h3>
			<p>Я регулярно вижу в сети, как некоторые проводят сравнения систем больших данных: 
				<br>они берут набор своих данных, каким-либо образом “скармливают” его оцениваемой 
				<br>системе, а затем немедленно пытаются измерить производительность - сколько 
				<br>памяти или дискового пространства было занято, и насколько быстро выполнялись 
				<br>запросы. Причем понимание того, как устроены изнутри испытываемые ими системы, у 
				<br>них отсутствует. Затем, используя лишь подобные специфичные данные о 
				<br>производительности - иногда вместе со списками функциональности, которая им 
				<br>нужна и которая есть в системе <i>на настоящий момент,</i> - они в итоге делают свой 
				<br>выбор или, что еще хуже, выбирают написать свою собственную, “лучшую” систему с 
				<br>нуля.</p>
			<p>Такой подход мне кажется неправильным, по крайней мере он неприменим в 
				<br>отношении открытых OLAP-систем для Big Data. Задача создания системы Bid Data 
				<br>OLAP, которая смогла бы работать эффективно в большинстве сценариев 
				<br>использования и содержала бы все необходимые функции настолько велика, что я 
				<br>оцениваю ее реализацию как минимум в <b>100 человеко-лет</b>.</p>
			<p>На сегодня, ClickHouse, Druid и Pinot оптимизированы <i>только</i> для конкретных 
				<br>сценариев использования, которые требуются их разработчиком - и содержат по 
				<br>большей части лишь те функции, в которых нуждаются сами разработчики. Я могу 
				<br>гарантировать, что ваш случай обязательно “упрется” в те узкие места, с которыми 
				<br>разработчики рассматриваемых OLAP-систем еще не сталкивались - или же в те 
				<br>места, что их не интересуют. </p>
			<p>Не говоря уже о том, что упомянутый выше подход “забросить данные в систему, о 
				<br>которой вы ничего не знаете, и затем измерить её эффективность” весьма вероятно 
				<br>даст искаженный результат из-за серьезных “узких” мест, которые на самом деле 
				<br>могли бы быть исправлены <b>простым изменением конфигурации</b>, схемы данных или 
				<br>другим построением запроса.</p>
		<h3>CloudFlare: ClickHouse против Druid</h3>
			<p>Одним таким примером, хорошо иллюстрирующим описанную выше проблему, 
				<br>является пост Марека Вавруша о <a href="">выборе между ClickHouse и Druid в Cloudflare</a>. Им 
				<br>потребовалось 4 сервера ClickHouse (которые со временем превратились в 9), и по их 
				<br>оценкам, для разворачивания аналогичной установки Druid им бы потребовались 
				<br>“сотни узлов”. Пусть Марек и признает, что <b>сравнение является нечестным</b>, 
				<br>поскольку Druid недостаёт “сортировки по первичному ключу”, он возможно даже не 
				<br>осознает, что достичь примерно того же самого эффекта в Druid возможно просто 
				<br><a href="">установив правильный порядок измерений в “ingestion spec”</a> и произведя простую 
				<br>подготовку данных: обрезать значение колонки __time в Druid до некоей грубой 
				<br>детализации (к примеру, один час) и опционально добавить другую “длинно-типовую” 
				<br>колонку “precise_time”, если для некоторых запросов требуются более точные 
				<br>временные рамки. Да, это хак, но, как мы только что выяснили, и в Druid можно 
				<br>сортировать данные по какому-либо измерению перед __time, и это достаточно 
				<br>просто реализовать.</p>
			<p>Впрочем, я не стану спорить с их итоговым решением выбрать ClickHouse, поскольку 
				<br>на масштабе примерно в 10 узлов и для их нужд ClickHouse мне тоже кажется лучшим 
				<br>выбором, чем Druid. Но сделанное ими заключение о том, что ClickHouse как минимум 
				<br>на порядок эффективнее (по меркам стоимости инфраструктуры), чем Druid - это 
				<br>серьезное заблуждение. На самом деле, из рассматриваемых нами сегодня систем, 
				<br><b>Druid предлагает наилучшую возможность для реально дешевых установок</b> 
				<br>(смотрите раздел “Уровни узлов обработки запросов в Druid ” ниже).</p>
			<p><blockquote><span style="background-color: yellow">Когда вы выбираете систему OLAP Big Data, не сравнивайте то, насколько они 
				<br>сейчас хорошо подходят для вашего случая. Сейчас они все субоптимальны. 
				<br>Вместо этого, сравните, насколько быстро ваша компания способна заставить 
				<br>двигаться эти системы в том направлении, которое нужно именно вам.</span></blockquote></p>
			<p>В силу своей фундаментальной архитектурной схожести, ClickHouse, Druid и Pinot 
				<br>имеют примерно один и тот же “предел” эффективности и оптимизации 
				<br>производительности. Здесь нет “волшебной таблетки”, которая позволила бы какой-
				<br>либо из этих систем быть быстрее, чем остальные. Не позволяйте запутать себя тем 
				<br>фактом, что <i>в своем текущем состоянии</i> системы показывают себя очень по-разному 
				<br>в различных бенчмарках. </p>
			<p>Допустим, Druid не поддерживает “сортировку по первичному ключу” настолько 
				<br>хорошо, насколько это умеет ClickHouse - а ClickHouse в свою очередь не 
				<br>поддерживает “инвертированные индексы” столь же хорошо, как Druid, что дает 
				<br>данным системам преимущества с той или иной нагрузкой. <b>Упущенные оптимизации 
				<br>могут быть реализованы в выбранной системе при помощи не таких уж и 
				<br>больших усилий</b>, если у вас есть намерение и возможность решиться на подобный 
				<br>шаг.</p>
					<ul>
						<li>В вашей организации должны быть инженеры, способные прочитать, понять и 
							<br>модифицировать исходный код выбранной системы, к тому же у них должно 
							<br>быть на это время. Заметьте, что ClickHouse написан на C++, а Druid и Pinot?—? 
							<br>на Java.</li>
						<br>
						<li>Или же ваша организация должна подписать контракт с компанией, которая 
							<br>оказывает поддержку выбранной системы. Это будут <a href="">Altinity</a> для ClickHouse, 
							<br><a href="">Imply</a> и <a href="">Hortonworks</a> для Druid. Для Pinot таких компаний в данный момент нет.</li>					
					</ul>
					<p>Другие сведения о разработке систем, которые вам стоит принять во внимание:</p>
					<ul>	
						<li>Авторы ClickHouse, работающие в Yandex, утверждают, что они тратят 50% 
							<br>своего времени на создание функциональности, которая требуется им внутри 
							<br>компании, и другие 50% уходят на функции, который набирают большинство 
							<br>“голосов сообщества”. Однако, чтобы вы получили от этого факта 
							<br>преимущество, требуется, чтобы <b>функции, которые нужны вам, были и 
							<br>наиболее востребованы сообществом</b> ClickHouse.</li>
						<li>Разработчики Druid из Imply мотивированы работать над широко 
							<br>используемыми функциями, поскольку это позволит им максимально увеличить 
							<br>объем охвата своего бизнеса в будущем.</li>
						<li>Процесс разработки Druid сильно напоминает <a href="">модель Apache</a>, когда ПО 
							<br>несколько лет разрабатывается несколькими компаниями, у каждой из который 
							<br>достаточно своеобразные и различные приоритеты, и среди них нет ведущей 
							<br>компании. ClickHouse и Pinot пока еще далеки от подобного этапа, поскольку 
							<br>ими занимаются соответственно лишь Yandex и Linkedin. Сторонний вклад в 
							<br>развитие Druid имеет минимальный шанс быть отклоненным в силу того, что он 
							<br>расходится с видением основного разработчика - ведь <b>в Druid нет “основной” 
							<br>компании-разработчика</b>.</li>
						<li>Druid поддерживает “API разработчика”, который позволяет привносить 
							<br>собственные типы колонок, механизмы агрегации, возможные варианты для 
							<br>«глубокого хранения» и пр., причем все это вы можете держать в кодовой базе, 
							<br>отдельной от самого ядра Druid. Данное API документировано разработчиками 
							<br>Druid, и они следят за его совместимостью с предыдущими версиями. Однако, 
							<br>оно недостаточно “взрослое”, и ломается практически с каждым новым релизом 
							<br>Druid. Насколько мне известно, в ClickHouse и Pinot схожие API не 
							<br>поддерживаются.</li>
						<li>Согласно Github, <b>над Pinot работает наибольшее число людей</b> - по всей 
							<br>видимости, лишь за прошлый год в Pinot было вложено <a href="">не менее 10 человеко-
							<br>лет</a>. Для ClickHouse эта цифра составляет примерно 6 человеко-лет, а для 
							<br>Druid - 7. В теории, это должно означать, что Pinot улучшается быстрее всех 
							<br>остальных систем, которые мы рассматриваем.</li>
					</ul>
					<p>Архитектуры Druid и Pinot почти что идентичны друг другу, в то время как ClickHouse 
						<br>стоит слегка в стороне. Поэтому сначала мы сравним ClickHouse c “обобщенной” 
						<br>архитектурой Druid/Pinot, а затем обсудим мелкие различия между Druid и Pinot.</p>
		<h2>Различия между ClickHouse и Druid/Pinot</h2>
			<h3>Управление данными: Druid и Pinot</h3>
				<p>В Druid и Pinot, все данные в каждой “таблице” (как бы она не называлась в 
					<br>терминологии этих систем) разбиваются на указанное количество частей. По 
					<br>временой оси, данные обычно разделены с заданым интервалом. Затем эти части 
					<br>данных «запечатываются» индивидуально в самостоятельные автономные сущности, 
					<br>называемые «сегментами». Каждый сегмент включает в себя метаданные таблицы, 
					<br>сжатые столбчатые данные и индексы.</p>
				<p>Сегменты хранятся в файловой системе хранилища «глубокого хранения» (например, 
					<br>HDFS) и могут быть загружены на узлы обработки запросов, но последние не отвечают 
					<br>за устойчивость сегментов, поэтому узлы обработки запросов могут быть заменены 
					<br>относительно свободно. <b>Сегменты не привязаны жестко к конкретным узлам</b> и 
					<br>могут быть загружены на те или другие узлы. Специальный выделенный сервер 
					<br>(который называется “координатором” в Druid и “контроллером” в Pinot, но я ниже 
					<br>обращаюсь к нему как к “мастеру”) отвечает за присвоение сегментов узлам, и 
					<br>перемещению сегментов между узлами, если потребуется.</p>
				<p>Это не противоречит тому, что я отмечал выше, все три системы имеют статическое 
					<br>распределение данных между узлами, поскольку загрузки сегментов и их 
					<br>перемещения в Druid - и насколько я понимаю в Pinot - являются дорогими 
					<br>операциями и потому не выполняются для каждой отдельной очереди, а происходят 
					<br>обычно раз в несколько минут/часов/дней.</p>
				<p>Метаданные сегментов хранятся в ZooKeeper - напрямую в случае Druid, и при 
					<br>помощи фреймворка <a href="">Helix</a> в Pinot. В Druid метаданные также хранятся в базе SQL, об 
					<br>этом будет подробнее в разделе “Различия между Druid и Pinot”.</p>
			<h3>Управление данными: ClickHouse</h3>
				<p>В ClickHouse нет “сегментов”, содержащих данные, попадающие в конкретные 
					<br>временные диапазоны. В нем нет “глубокого хранения” для данных, узлы в кластере 
					<br>ClickHouse также отвечают и за обработку запросов, и за постоянство/устойчивость 
					<br>данных, хранящихся на них. Так что вам <b>не потребуется HDFS</b> или облачное 
					<br>хранилище данных вроде Amazon S3.</p>
				<p>В ClickHouse имеются секционированные таблицы, состоящие из указанного набора 
					<br>узлов. Здесь нет “центральной власти” или сервера метаданных. Все узлы, между 
					<br>которыми разделена та или иная таблица, содержат полные, идентичные копии 
					<br>метаданных, включая адреса всех остальных узлов, на которых хранятся секции этой 
					<br>таблицы.</p>
				<p>Метаданные секционированной таблицы включают “весы” узлов для распределения 
					<br>свежезаписываемых данных - к примеру, 40% данных должны идти на узел A, 30% на 
					<br>узел B и 30% на C. Обычно же распределение должно происходить равномерно, 
					<br>“перекоос”, как в этом примере, требуется только тогда, когда к секционированной 
					<br>таблице добавляется новый узел и нужно побыстрее заполнить его какими-либо 
					<br>данными. <b>Обновления этих “весов” должны выполняться вручную</b> 
					<br>администраторами кластера ClickHouse, или же автоматизированной системой, 
					<br>построенной поверх ClickHouse.</p>
			<h3>Управление данными: сравнение</h3>
				<p>Подход к управлению данными в ClickHouse проще, чем в Druid и Pinot: не требуется 
					<br>“глубокого хранилища”, всего один тип узлов, не требуется выделенного сервера для 
					<br>управления данными. Но подход ClickHouse приводит к некоторым трудностям, когда 
					<br>любая таблица данных вырастает настолько большой, что требуется ее разбиение 
					<br>между десятком или более узлов: коэффициент усиления запроса становится 
					<br>настолько же велик, насколько и фактор секционирования - даже для запросов, 
					<br>которые покрывают небольшой интервал данных:</p>
				<figure>	
					<img src="d:\HTML_practice\1-14.png"/>
					<br>
					<figcaption><blockquote><blockquote><blockquote><blockquote><i>Компромисс распределения данных в ClickHouse</i></blockquote></blockquote></blockquote></blockquote></figcaption>
				</figure>
				<p>В примере, показанном на изображении выше, данные таблицы распределены между 
					<br>тремя узлами в Druid/Pinot, но запрос по малому интервалу данных обычно 
					<br>затрагивает лишь два из них (до той поры, пока интервал не пересечет пограничный 
					<br>интервал сегмента). В ClickHouse, любые запросы будут вынуждены затронуть три 
					<br>узлв – если таблица сегментирована между тремя узлами. В данном примере разница 
					<br>не выглядит настолько существенно, однако представьте себе, что случится, если 
					<br>число узлов достигнет 100 – в то время как фактор сегментирования по-прежнему 
					<br>может быть равен, например, 10 в Druid/Pinot.</p>
				<p>Чтобы смягчить эту проблему, самый большой кластер ClickHouse в Яндексе, 
					<br>состоящий из сотен узлов, в действительности разбит на многие «под-кластеры» с 
					<br>несколькими десятками узлов в каждом. Кластер ClickHouse используется в работе с 
					<br>аналитикой по веб-сайтам, и каждая точка данных имеет измерение «ID вебсайта». 
					<br>Существует жесткая привязка каждого ID сайта к конкретному под-кластеру, куда идут 
					<br>все данные для этого идентификатора сайта. Поверх кластера ClickHouse есть слой 
					<br>бизнес-логики, который управляет этим разделением данных при поглощении данных 
					<br>и выполнении запросов. К счастью, в их сценариях использования совсем немного 
					<br>запросов затрагивают несколько идентификаторов сайтов, и подобные запросы идут 
					<br>не от пользователей сервиса, поэтому у них нет жесткой привязки к реальному 
					<br>времени согласно соглашению об уровне услуг.</p>
				<p>Другим недостатком подхода ClickHouse является то, что, когда кластер растет очень 
					<br>быстро, данные не могут перебалансироваться автоматически без участия человека, 
					<br>который вручную поменяет «веса» узлов в разбиваемой таблице.</p>
			<h3>Уровни узлов обработки запросов в Druid</h3>
				<p>Управление данными при помощи сегментов «проще себе представить» - эта 
					<br>концепция хорошо ложится на наши когнитивные способности. Сами сегменты можно 
					<br>перемещать между узлами относительно просто. Эта две причины позволили Druid 
					<br>реализовать <i>“разделение на уровни”</i> узлов, занимающихся обработкой запросов: 
					<br>старые данные автоматически перемещаются на сервера с относительно большими 
					<br>дисками, но меньшим количеством памяти и CPU, что позволяет <b>значительно 
					<br>снизить стоимость большого рабочего кластера Druid</b> за счет замедления 
					<br>запросов к более старым данным.</p>
				<p>Эта функция позволяет Metamarkets экономить сотни тысяч долларов расходов на 
					<br>инфраструктуру Druid каждый месяц - в противовес тому варианту, если бы 
					<br>использовался “плоский” кластер.</p>
				<figure>
					<img src="d:\HTML_practice\2-14.png"/>
					<br>
					<figcaption><blockquote><blockquote><blockquote><blockquote><i>Уровни узлов обработки запросов в Druid</figcaption></blockquote></blockquote></blockquote></blockquote></i></figcaption>
				</figure>
				<p>Насколько мне известно, в ClickHouse и Pinot пока еще нет похожей функциональности 
					<br>- предполагается, что все узлы в их кластерах одинаковы.</p>
				<p>В силу того, что архитектура Pinot весьма схожа с архитектурой Druid, как мне кажется, 
					<br>будет не слишком сложно добавить аналогичную функцию в Pinot. Тяжелее будет в 
					<br>случае с ClickHouse, поскольку для реализации данной функции крайне полезно 
					<br>использование концепта “сегментов”, однако это всё равно возможно.</p>
			<h3>Репликация данных: Druid и Pinot</h3>
				<p>Единицей репликации в Druid и Pinot является единичный сегмент. Сегменты 
					<br>реплицируются на уровне «глубокого хранения» (например, в три реплики на HDFS, 
					<br>или при помощи хранилища BLOB-объектов в Amazon S3), и на уровне обработки 
					<br>запросов: обычно и в Druid и в Pinot, каждый сегмент загружается на два различных 
					<br>узла. «Мастер»-сервер мониторит уровни репликации для каждого сегмента и 
					<br>загружает сегмент на какой-либо сервер, если фактор репликации падает ниже 
					<br>заданного уровня (например, если какой-либо из узлов перестаёт отвечать).</p>
			<h3>Репликация данных: ClickHouse</h3>
				<p>Единицей репликации в ClickHouse является секция таблицы на сервере (например, 
					<br>все данные из какой-либо таблицы, хранящиеся на сервере). Аналогично 
					<br>секционированию, репликация в ClickHouse является скорее «статической и 
					<br>конкретной», чем «в облачном стиле»: несколько серверов знают, что они являются 
					<br>репликами друг друга (для некоторой конкретной таблицы; в случае другой таблицы, 
					<br>конфигурация репликации может отличаться). Репликация предоставляет и 
					<br>устойчивость, и доступность запросов. Когда повреждается диск на одном узле, 
					<br>данные не теряются, поскольку они хранятся еще и на другом узле. Когда какой-либо 
					<br>узел временно недоступен, запросы могут быть перенаправлены на реплику.</p>
				<p>В самом большом кластере ClickHouse в Яндексе есть два одинаковых набора узлов в 
					<br>различных дата-центрах, и они спарены. В каждой паре узлы являются репликами 
					<br>друг друга (используется фактор репликации, равный двум), и они расположены в 
					<br>различных дата-центрах.</p>
				<p>ClickHouse полагается на ZooKeeper для управления репликацией – поэтому, если вам 
					<br>не нужна репликация, то вам не нужен и ZooKeeper. Это означает, что ZooKeeper не 
					<br>потребуется и для ClickHouse, развернутого на одиночном узле.</p>
			<h3>Поглощение данных: Druid и Pinot</h3>
				<p>В Druid и Pinot узлы обработки запросов специализируются на загрузке сегментов и 
					<br>обслуживают запросы к данным в сегментах; они не занимаются накоплением новых 
					<br>данных и производством новых сегментов.</p>
				<p>Когда таблица может обновляться с задержкой в час или более, сегменты создаются 
					<br>при помощи движков пакетной обработки – к примеру, Hadoop или Spark. И в Druid, и в 
					<br>Pinot есть первоклассная поддержка Hadoop из коробки. Существует <a href="">сторонний плагин 
					<br>для поддержки индексации Druid в Spark</a>, но в данный момент официально он не 
					<br>поддерживается. Насколько мне известно, в Pinot такого уровня поддержки Spark пока 
					<br>нет, то есть вы должны быть готовы разобраться с интерфейсами Pinot и кодом, а 
					<br>затем самостоятельно написать код на Java/Scala, пусть это и не должно быть 
					<br>слишком сложно. (Впрочем, с момента публикации оригинальной статьи поддержка 
					<br>Spark в Pinot <a href="">была внесена контрибьютором</a>).</p>
				<p>Когда таблица должна обновляться в реальном времени, здесь приходит на помощь 
					<br>идея “реалтаймовых» узлов, которые делают три вещи: принимает новые данные из 
					<br>Kafka (Druid поддерживает и другие источники), обслуживает запросы с недавними 
					<br>данными, создает сегменты в фоне и затем записывает их в “глубокое хранилище”.</p>
			<h3>Поглощение данных: ClickHouse</h3>
				<p>Тот факт, что ClickHouse не требуется готовить “сегменты”, содержащие все данные и 
					<br>попадающие в заданные временные интервалы, позволяет строить более простую 
					<br>архитектуру поглощения данных. ClickHouse не требуется ни пакетный движок 
					<br>обработки вроде Hadoop, ни “реалтаймовые” узлы. Обычные узлы ClickHouse - те же 
					<br>самые, что занимаются хранением данных и обслуживают запросы к ним - напрямую 
					<br>принимают пакетные записи данных.</p>
				<p>Если таблица разбита на сегменты, то узел, который принимает пакетную запись 
					<br>(например, 10к строк) распределяет данные согласно “весам” (смотрите раздел ниже). 
					<br>Строки записываются одним пакетом, который формирует небольшое “множество”. 
					<br>Множество немедленно конвертируется в колоночный формат. На каждом узле 
					<br>ClickHouse работает фоновый процесс, который объединяет наборы строк в еще 
					<br>большие наборы. Документация ClickHouse сильно завязана на принцип, известный 
					<br>как “MergeTree”, и подчеркивает схожесть его работы с <a href="">LSM-деревом</a>, хотя меня это 
					<br>слегка смущает, поскольку данные не организованы в деревья - они лежат в плоском 
					<br>колончатом формате.</p>
			<h3>Поглощение данных: сравнение</h3>
				<p>Поглощение данных в Druid и Pinot является “тяжелым”: оно состоит из нескольких 
					<br>различных сервисов, и управление ими - это тяжелый труд.</p>
				<p>Поглощение данных в ClickHouse гораздо проще (что компенсируется сложностью 
					<br>управления “историческими” данными - т.е. данными не в реальном времени), но и 
					<br>здесь есть один момент: вы должны иметь возможность собирать данные в пакеты до 
					<br>самого ClickHouse. Автоматическое поглощение и пакетный сбор данных из <a href="">Kafka 
					<br>доступно “из коробки”</a>, но если у вас используется другой источник данных в реальном 
					<br>времени (здесь подразумевается всё, что угодно, в диапазоне между инфраструктурой 
					<br>запросов, альтернативной Kafka, и стриминговых движков обработки, вплоть до 
					<br>различных HTTP-endpoint), то вам придется создать промежуточный сервис по сбору 
					<br>пакетов, или же внести код напрямую в ClickHouse.</p>
			<h3>Выполнение запроса</h3>
				<p>В Druid и Pinot имеется отдельный слой узлов, называемых <i>“брокерами”</i>, которые 
					<br>принимают все запросы к системе. Они определяют, к каким “историческим” 
					<br><i>(содержащим данные не в реальном времени)</i> узлам обработки запросов должны быть 
					<br>отправлены подзапросы, основываясь на отображении сегментов в узлы, в которых 
					<br>сегменты загружаются. Брокеры хранят информацию об отображении в памяти. 
					<br>Брокер-узлы отправляют дальше подзапросы к узлам обработки запросов, и когда 
					<br>результаты этих подзапросов возвращаются, брокер объединяет их и возвращает 
					<br>финальный комбинированный результат пользователю.</p>
				<p>Я не берусь предполагать, зачем при проектировании Druid и Pinot было принято 
					<br>решение о введении еще одного типа узлов. Однако, теперь они кажутся их 
					<br>неотъемлемой частью, поскольку, когда общее количество сегментов в кластере 
					<br>начинает превышать десять миллионов, информация об отображении сегментов в 
					<br>узлы начинает занимать гигабайты памяти. Это очень расточительно – выделять 
					<br>столько много памяти на каждом узле для обработки запросов. Вот вам и еще один 
					<br>недостаток, который накладывается на Druid и Pinot их «сегментированной» 
					<br>архитектурой управления данными.</p>
				<p>В ClickHouse выделять отдельный набор узлов под “брокер запросов” обычно не 
					<br>требуется. Существует специальный, эфемерный <a href="">“распределенный” тип таблицы</a> в 
					<br>ClickHouse, который может быть установлен на любом узле, и запросы к этой таблице 
					<br>будут делать все то же, за что отвечают брокер-узлы в Druid и Pinot. Обычно подобные 
					<br>эфемерные таблицы размещаются на каждом узле, который участвует в 
					<br>секционированной таблице, так что на практике каждый узел может быть “входной 
					<br>точкой” для запроса в кластер ClickHouse. Этот узел может выпускать необходимые 
					<br>подзапросы к другим секциями, обрабатывать свою часть запроса самостоятельно и 
					<br>затем объединять её с частичными результатами от других секций.</p>
				<p>Когда узел (или один из процессинговых узлов в ClickHouse, или брокер-узел в Druid и 
					<br>Pinot) выпускает подзапросы к другим, и один или несколько подзапросов по какой-
					<br>либо причине заканчиваются неудачей, ClickHouse и Pinot обрабатывают эту ситуацию 
					<br>правильно: они объединяют результаты успешно выполненных подзапросов вместе, и 
					<br>всё равно возвращают частичный результат пользователю. <a href="">Druid этой функции сейчас 
					<br>очень недостает</a>: если в нем выполнение подзапроса заканчивается неудачей, то 
					<br>неудачей закончится и весь запрос целиком.</p>
			<h3>ClickHouse vs. Druid или Pinot: Выводы</h3>
				<p>“Сегментированный” подход к управлению данными в Druid и Pinot против более 
					<br>простого управления данными в ClickHouse определяет многие аспекты систем. 
					<br>Однако, важно заметить, что это различие оказывает небольшое (или не оказывает 
					<br>вовсе) влияние на потенциальную эффективность сжатия (впрочем, история про 
					<br>компрессию для всех трех систем имеет печальный конец по нынешнему состоянию 
					<br>дел), или на скорость обработки запросов.</p>
				<p>ClickHouse похож на традиционные RDMBS, например, PostgreSQL. В частности, 
					<br><b>ClickHouse</b> можно развернуть на всего один сервер. Если планируемый размер 
					<br>невелик - скажем, не больше порядка 100 ядер CPU для обработки запросов и 1 TB 
					<br>данных, я бы сказал, что ClickHouse имеет значительные преимущества перед Druid и 
					<br>Pinot в силу своей простоты и отсутствия необходимости в дополнительных типах 
					<br>узлов, таких как “мастер”, “узлы поглощения в реальном времени”, “брокеры”. На этом 
					<br>поле, ClickHouse соревнуется скорее с InfluxDB, чем с Druid или Pinot.</p>
				<p><b>Druid and Pinot</b> похож на системы Big Data вроде HBase. Здесь в виду имеются не 
					<br>характеристики производительности, а зависимость от ZooKeper, зависимость от 
					<br>персистентного реплицируемого хранилища (к примеру, HDFS), сосредоточение 
					<br>внимания на устойчивости к отказам отдельных узлов, а также автономная работа и 
					<br>управление данными, не требующими постоянного внимания человека.</p>
				<p>Для широкого спектра приложений, ни ClickHouse, ни Druid или Pinot не являются 
					<br>очевидными победителями. В первую очередь, вы должны принимать во внимание 
					<br>вашу способность разобраться с исходным кодом системы, исправлять баги, 
					<br>добавлять новые функции и т.д. Это подробнее обсуждается в разделе “Про 
					<br>сравнение производительности и выбор системы”.</p>
				<p>Во-вторых, вам стоит взглянуть на таблицу ниже. Каждая ячейка в этой таблице 
					<br>описывает свойство приложения, которое позволит определить предпочтительную 
					<br>систему. Строки отсортированы не в порядке важности. Важность различных свойств 
					<br>может разниться от приложения к приложению, но в целом можно применить 
					<br>следующий подход: если ваше приложение соответствует подавляющему 
					<br>большинству строк со свойствами в одной из колонок, то относящаяся к ней система в 
					<br>вашем случае является предпочтительным выбором.</p>
				<table border="1">
					<thead>
						<tr>
							<th>ClickHouse</th>
							<th>Druid или Pinot</th>
						</tr>
					</thead>
						<tr>
							<td>В организации есть эксперты по C++</td>
							<td>В организации есть эксперты по Java</td>
						</tr>
						<tr>
							<td>Малый кластер</td>
							<td>Большой кластер</td>
						</tr>
						<tr>
							<td>Немного таблиц</td>
							<td>Много таблиц</td>
						</tr>
						<tr>
							<td>Один набор данных</td>
							<td>Несколько несвязанных наборов данных</td>
						</tr>
						<tr>
							<td>Таблицы и данные находятся в кластере 
								<br>перманентно</td>
							<td>Таблицы и наборы данных периодически 
								<br>появляются в кластере и удаляются из 
								<br>него</td>
						</tr>
						<tr>
							<td>Размер таблиц (и интенсивность 
								<br>запросов к ним) остается стабильным во 
								<br>времени</td>
							<td>Таблицы значительно растут и 
								<br>сжимаются</td>
						</tr>
						<tr>
							<td>Однородные запросы (их тип, размер, 
								<br>распределение по времени суток и т.д.)</td>
							<td>Разнородные запросы</td>
						</tr>
						<tr>
							<td>В данных есть измерение, по которому 
								<br>оно может быть сегментировано, и почти 
								<br>не выполняется запросов, которые 
								<br>затрагивают данные, расположенные в 
								<br>нескольких сегментах</td>
							<td>Подобного измерения нет, и запросы 
								<br>часто затрагивают данные, 
								<br>расположенные во всем кластере</td>
						</tr>
						<tr>
							<td>Облако не используется, кластер должен 
								<br>быть развернут на специфическую 
								<br>конфигурацию физических серверов</td>
							<td>Кластер развернут в облаке</td>
						</tr>
						<tr>
							<td>Нет существующих кластеров Hadoop 
								<br>или Spark</td>
							<td>Кластеры Hadoop или Spark уже 
								<br>существуют и могут быть использованы</td>
						</tr>
				</table>
				<p><b>Примечание:</b> ни одно из свойств выше не означает, что вы должны использовать 
					<br>соответствующую систему (системы), или избегать другую. К примеру, если 
					<br>планируется, что ваш кластер будет большим, это не значит, что вы обязательно 
					<br>должны рассматривать только Druid или Pinot, исключив ClickHouse. Скорее всего, в 
					<br>данной ситуации Druid или Pinot могут быть лучшим выбором, но другие полезные 
					<br>свойства могут перевесить чашу весов в сторону ClickHouse, который для некоторых 
					<br>приложений является оптимальным выбором даже для больших кластеров.</p>
		<h2>Различия между Druid и Pinot</h2>
			<p>Как уже не раз отмечалось в данной статье, Druid и Pinot имеют весьма похожие 
				<br>архитектуры. Есть несколько достаточно заметных особенностей, которые есть в 
				<br>одной системе и отсутствуют в другой, и областей, в которых каждая из систем 
				<br>развита гораздо сильнее другой. Тем не менее, всё, о чем я собираюсь упомянуть 
				<br>ниже, можно воспроизвести в другой системе, приложив разумное количество усилий.</p>
			<p>Между Druid и Pinot существует лишь <b>одно существенное различие</b>, которое 
				<br>слишком велико для того, чтобы от него избавились в обозримом будущем - это 
				<br><b>реализация управления сегментами</b> в мастер-ноде. Кстати, разработчики обеих 
				<br>систем наверняка не хотели бы этого делать в любом случае, поскольку оба подхода 
				<br>имеют свои “за” и “против” - среди них нет такого, который был бы лучше.</p>
			<h2>Управление сегментами в Druid</h2>
				<p>Мастер-нода в Druid (и ни один из узлов в Pinot) не отвечают за сохранность 
					<br>метаданных в сегментах данных в кластере, и текущее отображение между 
					<br>сегментами и узлами обработки данных, на которых загружены сегменты. Эта 
					<br>информация хранится в ZooKeeper. Однако, Druid <i>в дополнение</i> хранит эту 
					<br>информацию еще и в SQL базе данных, которая необходима для развертывания 
					<br>кластера Druid. Не могу сказать, с какой целью было принято такое решение, но 
					<br>сейчас оно дает следующие преимущества:</p>
				<ul>
					<li>В ZooKeeper <b>хранится меньше данных</b>. Только минимум информации об 
						<br>отображении идентификатора сегмента на список узлов, занимающихся 
						<br>обработкой запросов, куда загружен сегмент, сохраняется в ZooKeeper. 
						<br>Оставшиеся метаданные, к примеру, размер сегмента, список измерений и 
						<br>метрики, и т.д. - хранятся только в SQL базе данных.</li>
					<br>
					<li>Когда сегменты данных вытесняются из кластера, поскольку они становятся 
						<br>слишком старыми (это общая функция всех баз данных временных рядов - она 
						<br>есть и в ClickHouse, и в Druid, и в Pinot), они выгружаются из узлов обработки 
						<br>запросов и их метаданные удаляются из ZooKeeper, но не из “глубокого 
						<br>хранилища” и не из базы данных SQL. Пока они не будут удалены из этих мест 
						<br>вручную, <b>остается возможность “оживить” действительно старые данные 
						<br>быстро</b>, если он потребуются для построения отчетов или исследований.</li>
					<br>
					<li>Вряд ли это планировалось с самого начала, но теперь есть планы сделать 
						<br>зависимость Druid от ZooKeeper опциональной. Сейчас ZooKeeper 
						<br>используется для трех различных функций: управления сегментами, 
						<br>обнаружения сервисов и хранения свойств (например, для управления 
						<br>поглощением данных в реальном времени). Обнаружение сервисов может <a href="">быть 
						<br>предоставлено Consul</a>. Управление сегментами может быть реализовано <a href="">при 
						<br>помощи HTTP-команд</a>, и оно доступно нам благодаря тому, что функции 
						<br>хранения в ZooKeeper “бекапится” в базе SQL.</li>
				</ul>
				<p>То что нам приходится иметь в зависимостях базу данных SQL, приводит к большей 
					<br>нагрузке на эксплуатацию, особенно, если в компании еще не использовалась какая-
					<br>либо БД SQL. Druid поддерживает MySQL и PostgreSQL, есть и расширение для 
					<br>Microsoft SQL Server. Кроме того, когда Druid рразворачивается в облаке, можно 
					<br>использовать стандартные сервисы для управления RDBMS - к примеру, Amazon RDS.</p>
			<h3>Управление сегментами в Pinot</h3>
				<p>В отличие от Druid, который реализует всю логику управления сегментами 
					<br>самостоятельно и полагается только на <a href="">Curator</a> для взаимодействия с ZooKeeper, Pinot 
					<br>делегирует большую часть логики управления сегментами и кластерами на <a href="">фреймворк 
					<br>Helix</a>. </p>
				<p>С одной стороны, я могу понять, что это дает разработчикам Pinot возможность 
					<br>сосредоточиться на других частях их системы. В Helix возможно меньше багов, чем в 
					<br>логике внутри самого Druid, поскольку он тестируется в других условиях и поскольку в 
					<br>него, предположительно, было вложено гораздо больше рабочего времени.</p>
				<p>С другой стороны, Helix возможно ограничивает Pinot своими “рамками фреймворка”. 
					<br>Helix, и следовательно, <b>Pinot, скорее всего будут зависеть от ZooKeeper всегда</b>.</p>
				<p>Далее я собираюсь перечислить менее важные различия между Druid и Pinot - в том 
					<br>смысле, что если у вас возникнет серьезное желание повторить одну из этих функций 
					<br>в вашей системе, то это будет вполне осуществимо.</p>
			<h3>«Проталкивание предикатов» в Pinot</h3>
				<p>Если во время поглощения данные секционируются в Kafka по каким-либо ключам 
					<br>измерений, Pinot создает сегменты, которые содержат информацию об этом 
					<br>разбиении и затем, когда выполняется запрос с предикатом на данном измерении, 
					<br>брокер-узел фильтрует сегменты таким образом, чтобы как можно меньше сегментов 
					<br>и узлов обработки запросов было затронуто.</p>
				<p>Эта концепция в оригинале называется <b>“predicate pushdown”</b> и важна для 
					<br>поддержания высокой производительности в некоторых приложениях.</p>
				<p>На данный момент Druid поддерживает разбиение по ключам, если сегменты были 
					<br>созданы в Hadoop, но еще не поддерживает сегменты, созданные во время 
					<br>поглощения в реальном времени. Druid сейчас не реализует функцию «проталкивания 
					<br>предикатов» на брокеры.</p>
			<h3>“Сменный” Druid и своевольный Pinot</h3>
				<p>Поскольку Druid используют различные организации и в его разработке принимают 
					<br>участие несколько компаний, он обзавелся поддержкой нескольких взаимозаменяемых 
					<br>опций для практически любой выделенной части или “сервиса”:</p>
				<ul>
					<li>HDFS, Cassandra, Amazon S3, Google Cloud Storage или Azure Blob Storage и 
						<br>т.д. в качестве “глубокого хранилища”;</li>
					<li>Kafka, илиr RabbitMQ, Samza, или Flink, или Spark, Storm, и т.д. (через 
						<br><a href="">Tranquility</a>) в качестве источника поглощения данных в реальном времени;</li>
					<li>Сам Druid, или Graphite, или Ambari, или StatsD, или Kafka в качестве “слива” 
						<br>для телеметрии кластера Druid (метрик).</li>
				</ul>
				<p>В то же время Pinot почти целиком разрабатывался исключительно в стенах LinkedIn и 
					<br>должен был удовлетворять текущим нуждам компании, поэтому выбор, который вам 
					<br>предлагается, не так велик. В качестве “глубокого хранилища” необходимо 
					<br>использовать HDFS или Amazon S3, а для поглощения данных в реальном времени 
					<br>подойдет только Kafka. Но если кому-то это действительно понадобится, мне кажется, 
					<br>не составит особой сложности добавить поддержку любого другого сервиса в Pinot. К 
					<br>тому же, можно ожидать позитивных сдвигов в этом направлении, поскольку <a href="">Uber</a> и 
					<br>Slack начинают использовать Pinot.</p>
			<h3>Формат данных и движок выполнения запросов лучше <br>оптимизированы в Pinot</h3>
				<p>В частности, следующие функции <a href="">формата сегментов Pinot</a> сейчас отсутствуют в 
					<br>Druid:</p>
				<ul>
					<li><b>Сжатие проиндексированных столбцов</b> с битовой гранулярностью, но <br>байтовой гранулярностью в Druid.</li>
					<li><b>Инвертированный индекс опционален</b> для каждого столбца. В Druid он <br>является обязательным, иногда этого не требуется, но все равно занимает <br>много места. Различие в потреблении места между Druid и Pinot, на <a href="">которое <br>указывает Uber в своих тестах</a>, вполне возможно вызвано именно этим.</li>
					<li><b>Минимальные и максимальные значения</b> в числовых столбцах <br>записываются посегментно.</li>
					<li><b>Поддержка сортировки данных из коробки</b>. В Druid этого можно достичь <br>только вручную и слегка специфическим способом (как было описано в разделе <br>“CloudFlare: ClickHouse против Druid”). Сортировка данных означает лучшее <br>сжатие, и эта функция в Pinot - еще одна причина различия между Druid и Pinot <br>в потреблении пространства (и производительности запросов!), на которую <br>указывает Uber.</li>
					<li><b>Формат данных</b>, используемый для многозначных столбцов, на данный <br>момент лучше оптимизирован в Pinot, чем в Druid.</li>
				</ul>
				<p>Однако, все это можно реализовать и в Druid. И несмотря на то, что формат Pinot 
					<br>оптимизирован существенно лучше, чем формат Druid, он все равно достаточно далек 
					<br>от того, чтобы быть оптимальным. Один из примеров: Pinot (как и Druid) использует 
					<br>только сжатие общего назначение (как Zstd) и еще не реализовали идеи сжатия из 
					<br><a href="">Gorilla</a>.</p>
				<p>К сожалению Uber по большей части использовал запросы count (*) для сравнения 
					<br>производительности Druid и Pinot относительно выполнения запроса [<a href="">1, 2</a>], который 
					<br>сейчас в Druid представляет собой тупое линейное сканирование, хотя его и несложно 
					<br>заменить <a href="">корректной O(1) реализацией</a>. Это вам еще один пример бессмысленных 
					<br>сравнений в стиле “черного ящика”, о которых мы говорили ранее.</p>
				<p>По моему мнению, причины сильного различия в производительности запросов GROUP 
					<br>BY, которое наблюдали в Uber, стоит искать в недостатке сортировки данных в 
					<br>сегментах Druid, как уже было отмечено выше в этом разделе.</p>
			<h3>У Druid есть более умный алгоритм присваивания <br>(балансировки) сегментов</h3>
				<p>Алгоритм Pinot заключается в присвоении сегмента к узлам обработки запроса, 
					<br>которые имеют наименьшее число сегментов, загруженных в текущий момент. 
					<br>Алгоритм Druid является гораздо более сложным; он учитывает таблицу каждого 
					<br>сегмента и время, и применяет <b>сложную формулу для вычисления финального 
					<br>коэффициента</b>, согласно которому будут ранжированы узлы обработки запросов для 
					<br>выбора наилучшего, которому и будет присвоен новый сегмент. Этот алгоритм 
					<br>показал ускорение в скорости выполнения запросов в продакшне Metamarkets на <b>30-
					<br>40%</b>. Хотя, даже несмотря на подобный результат, мы им по-прежнему не слишком 
					<br>довольны - подробности можно прочитать в <a href="">отдельной статье</a>.</p>
				<p>Не знаю, как в LinkedIn управляются со всем при помощи настолько простого 
					<br>алгоритма балансировки сегментов в Pinot, но, вполне возможно, их ожидают 
					<br>значительные улучшения по части производительности, если они решатся потратить 
					<br>время на совершенствование используемого ими алгоритма.</p>
			<h3>Pinot более устойчив к отказам при выполнении сложных <br>запросов</h3>
				<p>Как уже упоминалось выше в разделе “Выполнение запроса”, когда брокер-узел 
					<br>создает подзапросы к другим узлам, некоторые подзапросы заканчиваются ошибкой, 
					<br>но Pinot объединяет результаты всех удачно выполненных подзапросов и по-
					<br>прежнему возвращает частичный результат пользователю.</p>
				<p>В Druid такой функции на данный момент.</p>
			<h3>Иерархия узлов обработки запросов в Druid</h3>
				<p>Смотрите аналогичный раздел выше. Druid позволяет вводить уровни узлов обработки 
					<br>запросов для старых и новых данных, и для узлов со “старыми” данными соотношение 
					<br>“ресурсы CPU, RAM / число загруженных сегментов” гораздо ниже, что позволяет 
					<br>выиграть на расходах на инфраструктуру в обмен на низкую производительность 
					<br>запросов при доступе к старым данным.</p>
				<p>Насколько мне известно, в Pinot на данный момент аналогичная функциональность <br>отсутствует.</p>
		<h2>Заключение</h2>
			<p><b>ClickHouse, Druid и Pinot имеют фундаментально схожую архитектуру</b>, и 
				<br>занимают свою собственную нишу между Big Data-фреймворками общего назначения 
				<br>вроде Impala, Presto, Spark, и колоночными базами данных с корректной поддержкой 
				<br>первичных ключей, точечных обновлений и удалений, как InfluxDB.</p>
			<p>В силу схожести архитектур, ClickHouse, Druid и Pinot имеют примерно одинаковый 
				<br>“предел оптимизации”. Но в своем текущем состоянии, <b>все три системы еще 
				<br>незрелы</b> и очень далеки от этого лимита. Существенных улучшений в 
				<br>производительности данных систем (применительно к специфическим сценариям 
				<br>использования) можно достичь несколькими человеко-месяцами работы опытных 
				<br>инженеров. </p>
			<p><blockquote><span style="background-color: yellow">Я бы не рекомендовал вам сравнивать производительность данных систем 
				<br>между собой - выберите для себя ту, чей исходный код вы способны понять и 
				<br>модифицировать, или ту, в которую вы хотите инвестировать свои ресурсы.</span></blockquote></p>
			<p>Из этих трех систем, ClickHouse стоит немного в стороне от Druid и Pinot - в то время 
				<br>как Druid и Pinot практически идентичны, и их можно считать двумя независимо 
				<br>разрабатываемыми реализациями одной и той же системы.</p>
			<p>ClickHouse больше напоминает “традиционные” базы данных вроде PostgreSQL. 
				<br>ClickHouse можно установить на один узел. При малых масштабах (менее 1 TB 
				<br>памяти, менее 100 ядер CPU), ClickHouse выглядит гораздо более интересным 
				<br>вариантом, чем Druid или Pinot - если вам все еще хочется их сравнивать - в силу того, 
				<br>что ClickHouse проще и имеет меньше движущихся частей и сервисов. Я бы даже 
				<br>сказал, что на таком масштабе он скорее становится конкурентом для InfluxDB или 
				<br>Prometheus, а не для Druid или Pinot.</p>
			<p>Druid и Pinot больше напоминают другие системы Big Data из экосистемы Hadoop. Они 
				<br>сохраняют свои “самоуправляемые” свойства даже на очень больших масштабах 
				<br>(более 500 узлов), в то время как ClickHouse потребует для этого достаточно много 
				<br>работы профессиональных SRE. Кроме того, Druid и Pinot занимают выигрышную 
				<br>позицию в плане оптимизации инфраструктурной стоимости больших кластеров, и 
				<br>лучше подходят для облачных окружений, чем ClickHouse.</p>
			<p>Единственным долгосрочным различием между Druid и Pinot является то, что Pinot 
				<br>зависит от фреймворка Helix и будет продолжать зависеть от ZooKeeper, в то время 
				<br>как Druid может уйти от зависимости от ZooKeeper. С другой стороны, установка Druid 
				<br>продолжит зависеть от наличия какой-либо SQL-базы данных. На данный момент, 
				<br>Pinot оптимизирован лучше, чем Druid.</p>
			<p><blockquote><span style="background-color:yellow">Если вы уже сталкивались с необходимостью сравнения этих систем и сделали <br>свой выбор, то приходите на одну из наших конференций и расскажите о своем <br>кейсе: о том какие именно были задачи и какие грабли (а наверняка они были) вы <br>встретили. Хотя, конечно, базы данных далеко не единственная тема.<br>Ближайший по окончанию срока подачи заявок (<b>до 9 апреля</b>) фестиваль <a href="">РИТ++</a> <br>включает направления: <a href="">фронтенд</a>, <a href="">бэкенд</a>, <a href="">эксплуатацию</a> и <a href="">управление</a>. <br>Участникам обычно интереснее всего узнать о конкретных примерах, но и <br>выступления и в виде обзоров и исследований тоже возможны – главное, чтобы <br>тема была интересна лично вам. </span></blockquote></p>
	</body>
</html>